"""
ë¬¸ì„œ ì„ë² ë”© ìŠ¤í¬ë¦½íŠ¸
FAQì™€ ìƒí’ˆ ì •ë³´ë¥¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì„ë² ë”©
"""
import json
import os
import sys
from pathlib import Path
from dotenv import load_dotenv
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

data_dir = project_root / "data"
vector_db_path = data_dir / "vectordb_chroma"

def load_documents():
    """ë¬¸ì„œ ë¡œë“œ"""
    documents = []
    
    # FAQ ë¬¸ì„œ ë¡œë“œ
    faq_path = data_dir / "raw_docs" / "faq_data.json"
    if faq_path.exists():
        with open(faq_path, 'r', encoding='utf-8') as f:
            faq_data = json.load(f)
            
        for faq in faq_data:
            doc = Document(
                page_content=f"ì§ˆë¬¸: {faq['question']}\në‹µë³€: {faq['answer']}",
                metadata={
                    "source": "faq",
                    "category": faq['category'],
                    "faq_id": faq.get('faq_id', faq.get('id', 'unknown')),
                    "keywords": faq.get('keywords', '')
                }
            )
            documents.append(doc)
    
    # ì œí’ˆ ì •ë³´ ë¬¸ì„œ ë¡œë“œ
    product_path = data_dir / "raw_docs" / "product_info.json"
    if product_path.exists():
        with open(product_path, 'r', encoding='utf-8') as f:
            product_data = json.load(f)
            
        for product in product_data:
            # ì‚¬ì–‘ ì •ë³´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            specs = product.get('specifications', {})
            specs_text = ", ".join([f"{k}: {v}" for k, v in specs.items()]) if isinstance(specs, dict) else str(specs)
            
            # íŠ¹ì§• ì •ë³´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            features = product.get('features', [])
            features_text = ", ".join(features) if isinstance(features, list) else str(features)
            
            doc = Document(
                page_content=f"ìƒí’ˆëª…: {product['name']}\nì„¤ëª…: {product['description']}\nì‚¬ì–‘: {specs_text}\níŠ¹ì§•: {features_text}\nê°€ê²©: {product['price']:,}ì›",
                metadata={
                    "source": "product",
                    "product_id": product['product_id'],
                    "category": product['category'],
                    "price": product['price'],
                    "keywords": str(product.get('keywords', ''))
                }
            )
            documents.append(doc)
    
    return documents

def create_vector_store():
    """ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
    print("ğŸ“„ ë¬¸ì„œ ë¡œë“œ ì¤‘...")
    documents = load_documents()
    
    if not documents:
        print("âš ï¸ ë¡œë“œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    print(f"ğŸ“„ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
    
    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    print("ğŸ”§ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    embeddings = OpenAIEmbeddings()
    
    # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
    print("ğŸ’¾ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
    vector_db_path.mkdir(parents=True, exist_ok=True)
    
    vectorstore = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        persist_directory=str(vector_db_path)
    )
    
    # ì €ì¥
    vectorstore.persist()
    print(f"âœ… ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ: {vector_db_path}")
    
    return vectorstore

def test_search():
    """ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    embeddings = OpenAIEmbeddings()
    
    # ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
    vectorstore = Chroma(
        persist_directory=str(vector_db_path),
        embedding_function=embeddings
    )
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_queries = [
        "ë°°ì†¡ë¹„ëŠ” ì–¼ë§ˆì¸ê°€ìš”?",
        "ë¬´ì„  ì´ì–´í° ê°€ê²©",
        "ë°˜í’ˆ ë°©ë²•"
    ]
    
    for query in test_queries:
        print(f"\nì§ˆë¬¸: {query}")
        results = vectorstore.similarity_search(query, k=2)
        
        for i, doc in enumerate(results, 1):
            print(f"  ê²°ê³¼ {i}: {doc.page_content[:100]}...")
            print(f"  ì†ŒìŠ¤: {doc.metadata.get('source', 'unknown')}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ë¬¸ì„œ ì„ë² ë”© ì‹œì‘...")

    try:
        # RAG í”„ë¡œì„¸ì„œë¥¼ ì‚¬ìš©í•œ ì„ë² ë”©
        from core.rag_processor import RAGProcessor

        rag_processor = RAGProcessor()
        rag_processor.initialize_vector_store()

        print("âœ… ë¬¸ì„œ ì„ë² ë”© ì™„ë£Œ!")

        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰
        test_queries = [
            "ë°°ì†¡ë¹„ëŠ” ì–¼ë§ˆì¸ê°€ìš”?",
            "ë¬´ì„  ì´ì–´í° ì‚¬ì–‘ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
            "ë°˜í’ˆì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?"
        ]

        print("\nğŸ” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰:")
        for query in test_queries:
            print(f"\nì§ˆë¬¸: {query}")
            result = rag_processor.process_query(query)
            print(f"ë‹µë³€: {result['response'][:100]}...")
            print(f"ì‹ ë¢°ë„: {result['confidence']:.2f}")
            print(f"ì†ŒìŠ¤ ìˆ˜: {len(result['sources'])}")

        return 0

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main())
