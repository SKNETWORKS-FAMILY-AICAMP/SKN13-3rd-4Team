"""
<<<<<<< HEAD
RAG (Retrieval-Augmented Generation) í”„ë¡œì„¸ì„œ
ë²¡í„° DBë¥¼ ì‚¬ìš©í•œ ë¬¸ì„œ ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„±
"""
import json
import time
from pathlib import Path
from typing import Dict, Any, List, Optional

from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv

load_dotenv()


class RAGProcessor:
    """RAG ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± í´ëž˜ìŠ¤"""
    
    def __init__(self, model_name: str = "gpt-4o-mini"):
        self.model_name = model_name
        self.llm = ChatOpenAI(model=model_name, temperature=0.1)
        self.embeddings = OpenAIEmbeddings()
        self.vectorstore = None
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
        self.project_root = Path(__file__).parent.parent
        self.vector_db_path = self.project_root / "data" / "vectordb_chroma"
        self.raw_docs_path = self.project_root / "data" / "raw_docs"
=======
RAG (Retrieval-Augmented Generation) ì²˜ë¦¬ê¸°
FAQì™€ ìƒí’ˆ ì •ë³´ì— ëŒ€í•œ ë²¡í„° ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„±
"""
import json
from typing import List, Dict, Any
from pathlib import Path

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter

from dotenv import load_dotenv
load_dotenv()

class RAGProcessor:
    """RAG ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± í´ëž˜ìŠ¤"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.data_dir = self.project_root / "data"
        self.vector_db_path = self.data_dir / "vectordb_chroma"
        
        # OpenAI ëª¨ë¸ ì´ˆê¸°í™”
        self.llm = ChatOpenAI(
            model="gpt-4",
            temperature=0.1
        )
        
        # ìž„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embedding_model = OpenAIEmbeddings(
            model="text-embedding-3-large"
        )
        
        # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self.vector_store = None
        self.retriever = None
        
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸°
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
        )
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.rag_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ì‡¼í•‘ëª° ê³ ê° ì„œë¹„ìŠ¤ AI ì–´ì‹œìŠ¤í„´íŠ¸ìž…ë‹ˆë‹¤.
            
ì£¼ìš” ì—­í• :
- ê³ ê°ì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ì¹œì ˆí•œ ë‹µë³€ ì œê³µ
- ì œê³µëœ ë¬¸ì„œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±
- ì •ë³´ê°€ ë¶€ì¡±í•œ ê²½ìš° ì •ì¤‘í•˜ê²Œ ì•ˆë‚´

ë‹µë³€ ê°€ì´ë“œë¼ì¸:
1. ì¹œê·¼í•˜ê³  ì •ì¤‘í•œ ì–´ì¡° ì‚¬ìš©
2. êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì •ë³´ ì œê³µ
3. ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  ì•ˆë‚´
4. í•„ìš”ì‹œ ì¶”ê°€ ë¬¸ì˜ ë°©ë²• ì•ˆë‚´

ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´:
{context}
"""),
            ("human", "{question}")
        ])
>>>>>>> 98f88f8369a00fea011ba0112cbc9097e2eb5e55
    
    def initialize_vector_store(self):
        """ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
        try:
<<<<<<< HEAD
            if self.vector_db_path.exists():
                # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
                self.vectorstore = Chroma(
                    persist_directory=str(self.vector_db_path),
                    embedding_function=self.embeddings
                )
                print(f"âœ… ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ: {self.vector_db_path}")
            else:
                # ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
                print("âš ï¸ ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...")
                self._create_vector_store()
        except Exception as e:
            print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def _create_vector_store(self):
        """ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
        documents = self._load_documents()
        
        if not documents:
            raise ValueError("ë¡œë“œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        self.vector_db_path.mkdir(parents=True, exist_ok=True)
        self.vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=self.embeddings,
            persist_directory=str(self.vector_db_path)
        )
        
        # ì €ìž¥
        self.vectorstore.persist()
        print(f"âœ… ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ: {self.vector_db_path}")
    
    def _load_documents(self) -> List[Document]:
        """ë¬¸ì„œ ë¡œë“œ ë° Document ê°ì²´ ë³€í™˜"""
        documents = []
        
        # FAQ ë¬¸ì„œ ì²˜ë¦¬
        faq_documents = self._process_faq_documents()
        documents.extend(faq_documents)
        
        # ì œí’ˆ ì •ë³´ ë¬¸ì„œ ì²˜ë¦¬
        product_documents = self._process_product_documents()
        documents.extend(product_documents)
        
        return documents
    
    def _process_faq_documents(self) -> List[Document]:
        """FAQ ë¬¸ì„œ ì²˜ë¦¬"""
        documents = []
        faq_file = self.raw_docs_path / "faq_data.json"
        
        if not faq_file.exists():
            print(f"âš ï¸ FAQ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {faq_file}")
            return documents
        
        try:
=======
            # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹œë„
            self.vector_store = Chroma(
                persist_directory=str(self.vector_db_path),
                embedding_function=self.embedding_model,
                collection_name="ecommerce_docs"
            )
            
            # ë¬¸ì„œê°€ ìžˆëŠ”ì§€ í™•ì¸
            if self.vector_store._collection.count() == 0:
                print("ðŸ“„ ë²¡í„° ìŠ¤í† ì–´ê°€ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ìž„ë² ë”©í•©ë‹ˆë‹¤...")
                self._embed_documents()
            else:
                print(f"âœ… ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ (ë¬¸ì„œ ìˆ˜: {self.vector_store._collection.count()})")
            
            # ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
            self.retriever = self.vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 3}
            )
            
        except Exception as e:
            print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("ðŸ“„ ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
            self._create_new_vector_store()
    
    def _create_new_vector_store(self):
        """ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.vector_db_path.mkdir(parents=True, exist_ok=True)
        
        # ìƒˆ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        self.vector_store = Chroma(
            persist_directory=str(self.vector_db_path),
            embedding_function=self.embedding_model,
            collection_name="ecommerce_docs"
        )
        
        # ë¬¸ì„œ ìž„ë² ë”©
        self._embed_documents()
        
        # ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
        self.retriever = self.vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 3}
        )
    
    def _embed_documents(self):
        """ë¬¸ì„œë“¤ì„ ìž„ë² ë”©í•˜ì—¬ ë²¡í„° ìŠ¤í† ì–´ì— ì €ìž¥"""
        documents = []
        
        # FAQ ë¬¸ì„œ ì²˜ë¦¬
        faq_docs = self._process_faq_documents()
        documents.extend(faq_docs)
        
        # ìƒí’ˆ ì •ë³´ ë¬¸ì„œ ì²˜ë¦¬
        product_docs = self._process_product_documents()
        documents.extend(product_docs)
        
        if documents:
            # ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• 
            split_docs = self.text_splitter.split_documents(documents)
            
            # ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
            self.vector_store.add_documents(split_docs)
            print(f"âœ… {len(split_docs)}ê°œ ë¬¸ì„œ ì²­í¬ê°€ ë²¡í„° ìŠ¤í† ì–´ì— ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("âš ï¸ ìž„ë² ë”©í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    def _process_faq_documents(self) -> List[Document]:
        """FAQ ë°ì´í„°ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜"""
        documents = []
        faq_file = self.data_dir / "raw_docs" / "faq_data.json"
        
        if faq_file.exists():
>>>>>>> 98f88f8369a00fea011ba0112cbc9097e2eb5e55
            with open(faq_file, 'r', encoding='utf-8') as f:
                faq_data = json.load(f)
            
            for faq in faq_data:
                content = f"ì§ˆë¬¸: {faq['question']}\në‹µë³€: {faq['answer']}"
                doc = Document(
                    page_content=content,
                    metadata={
                        "source": "faq",
<<<<<<< HEAD
                        "category": faq.get('category', 'general'),
                        "faq_id": faq.get('id', ''),
                        "keywords": faq.get('keywords', [])
=======
                        "category": faq['category'],
                        "faq_id": faq['id'],
                        "keywords": faq['keywords']
>>>>>>> 98f88f8369a00fea011ba0112cbc9097e2eb5e55
                    }
                )
                documents.append(doc)
            
<<<<<<< HEAD
            print(f"âœ… FAQ ë¬¸ì„œ {len(documents)}ê°œ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ FAQ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
=======
            print(f"ðŸ“‹ FAQ ë¬¸ì„œ {len(documents)}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
>>>>>>> 98f88f8369a00fea011ba0112cbc9097e2eb5e55
        
        return documents
    
    def _process_product_documents(self) -> List[Document]:
<<<<<<< HEAD
        """ì œí’ˆ ì •ë³´ ë¬¸ì„œ ì²˜ë¦¬"""
        documents = []
        product_file = self.raw_docs_path / "product_info.json"
        
        if not product_file.exists():
            print(f"âš ï¸ ì œí’ˆ ì •ë³´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {product_file}")
            return documents
        
        try:
=======
        """ìƒí’ˆ ì •ë³´ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜"""
        documents = []
        product_file = self.data_dir / "raw_docs" / "product_info.json"
        
        if product_file.exists():
>>>>>>> 98f88f8369a00fea011ba0112cbc9097e2eb5e55
            with open(product_file, 'r', encoding='utf-8') as f:
                product_data = json.load(f)
            
            for product in product_data:
<<<<<<< HEAD
                # ì œí’ˆ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                content_parts = [
                    f"ì œí’ˆëª…: {product['name']}",
                    f"ê°€ê²©: {product['price']:,}ì›",
                    f"ì„¤ëª…: {product['description']}"
                ]
                
                if 'specifications' in product:
                    specs = product['specifications']
                    content_parts.append("ì‚¬ì–‘:")
                    for key, value in specs.items():
                        content_parts.append(f"- {key}: {value}")
                
                content = "\n".join(content_parts)
=======
                # ìƒí’ˆ ê¸°ë³¸ ì •ë³´
                content = f"ìƒí’ˆëª…: {product['name']}\n"
                content += f"ì¹´í…Œê³ ë¦¬: {product['category']}\n"
                content += f"ì„¤ëª…: {product['description']}\n"
                content += f"ê°€ê²©: {product['price']:,}ì›\n"
                
                # ì‚¬ì–‘ ì •ë³´
                if 'specifications' in product:
                    content += "\nì‚¬ì–‘:\n"
                    for key, value in product['specifications'].items():
                        content += f"- {key}: {value}\n"
                
                # íŠ¹ì§• ì •ë³´
                if 'features' in product:
                    content += "\nì£¼ìš” íŠ¹ì§•:\n"
                    for feature in product['features']:
                        content += f"- {feature}\n"
>>>>>>> 98f88f8369a00fea011ba0112cbc9097e2eb5e55
                
                doc = Document(
                    page_content=content,
                    metadata={
                        "source": "product",
<<<<<<< HEAD
                        "product_id": product.get('id', ''),
                        "category": product.get('category', ''),
                        "price": product.get('price', 0),
                        "name": product.get('name', '')
=======
                        "product_id": product['product_id'],
                        "category": product['category'],
                        "price": product['price'],
                        "keywords": product['keywords']
>>>>>>> 98f88f8369a00fea011ba0112cbc9097e2eb5e55
                    }
                )
                documents.append(doc)
            
<<<<<<< HEAD
            print(f"âœ… ì œí’ˆ ì •ë³´ ë¬¸ì„œ {len(documents)}ê°œ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ì œí’ˆ ì •ë³´ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return documents
    
    def process_query(self, query: str, k: int = 3) -> Dict[str, Any]:
        """
        ì¿¼ë¦¬ ì²˜ë¦¬ ë° ì‘ë‹µ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ìž ì§ˆë¬¸
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            
        Returns:
            ì‘ë‹µ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        
        if not self.vectorstore:
            return {
                "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "confidence": 0.0,
                "sources": [],
                "response_time": time.time() - start_time
            }
        
        try:
            # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
            search_results = self.vectorstore.similarity_search(query, k=k)
            
            if not search_results:
                return {
                    "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "confidence": 0.0,
                    "sources": [],
                    "response_time": time.time() - start_time
                }
            
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            context = self._format_search_results(search_results)
            
            # LLMì„ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±
            response = self._generate_response(query, context)
            
            # ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ
            sources = self._extract_source_info(search_results)
            
            # ì‹ ë¢°ë„ ê³„ì‚° (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
            confidence = self._calculate_confidence(search_results, query)
            
            return {
                "response": response,
                "confidence": confidence,
                "sources": sources,
                "response_time": time.time() - start_time
            }
            
        except Exception as e:
            return {
                "response": f"ì£„ì†¡í•©ë‹ˆë‹¤. ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "confidence": 0.0,
                "sources": [],
                "response_time": time.time() - start_time
            }
    
    def _format_search_results(self, search_results: List[Document]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìžì—´ë¡œ ë³€í™˜"""
        context_parts = []
        for i, doc in enumerate(search_results, 1):
            context_parts.append(f"[ì°¸ê³ ìžë£Œ {i}]\n{doc.page_content}")
        
        return "\n\n".join(context_parts)
    
    def _generate_response(self, query: str, context: str) -> str:
        """LLMì„ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±"""
        prompt_template = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì‡¼í•‘ëª° ê³ ê°ì„¼í„° ì§ì›ìž…ë‹ˆë‹¤.
ì•„ëž˜ ì°¸ê³ ìžë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³ ê°ì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì°¸ê³ ìžë£Œ:
{context}

ê³ ê° ì§ˆë¬¸: {query}

ë‹µë³€ ê°€ì´ë“œë¼ì¸:
1. ì°¸ê³ ìžë£Œì— ìžˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
2. ì •í™•í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì œê³µí•˜ì§€ ë§ˆì„¸ìš”
3. ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
4. í•„ìš”ì‹œ ì¶”ê°€ ë¬¸ì˜ë¥¼ ì•ˆë‚´í•˜ì„¸ìš”

ë‹µë³€:
""")
        
        try:
            messages = prompt_template.format_messages(
                context=context,
                query=query
            )
            
            response = self.llm.invoke(messages)
            return response.content.strip()
            
        except Exception as e:
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _extract_source_info(self, search_results: List[Document]) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ"""
        sources = []
        for doc in search_results:
            source_info = {
                "source": doc.metadata.get("source", "unknown"),
                "content_preview": doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content,
                "metadata": doc.metadata
            }
            sources.append(source_info)
        
        return sources
    
    def _calculate_confidence(self, search_results: List[Document], query: str) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚° (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)"""
        if not search_results:
            return 0.0
        
        # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì— ë”°ë¥¸ ê¸°ë³¸ ì‹ ë¢°ë„
        base_confidence = min(len(search_results) * 0.3, 0.9)
        
        # ì¿¼ë¦¬ í‚¤ì›Œë“œê°€ ê²°ê³¼ì— í¬í•¨ë˜ì–´ ìžˆëŠ”ì§€ í™•ì¸
        query_lower = query.lower()
        keyword_matches = 0
        
        for doc in search_results:
            content_lower = doc.page_content.lower()
            if any(word in content_lower for word in query_lower.split() if len(word) > 2):
                keyword_matches += 1
        
        # í‚¤ì›Œë“œ ë§¤ì¹˜ì— ë”°ë¥¸ ì‹ ë¢°ë„ ì¡°ì •
        keyword_confidence = keyword_matches / len(search_results) * 0.3
        
        return min(base_confidence + keyword_confidence, 1.0)


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # RAG í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
    rag = RAGProcessor()
    rag.initialize_vector_store()
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
    test_queries = [
        "ë°°ì†¡ë¹„ëŠ” ì–¼ë§ˆì¸ê°€ìš”?",
        "ë¬´ì„  ì´ì–´í° ë°°í„°ë¦¬ ì§€ì†ì‹œê°„ì€?",
        "ë°˜í’ˆ ì ˆì°¨ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"
    ]
    
    print("ðŸ” RAG í”„ë¡œì„¸ì„œ í…ŒìŠ¤íŠ¸ ì‹œìž‘\n")
    
    for query in test_queries:
        print(f"ì§ˆë¬¸: {query}")
        result = rag.process_query(query)
        
        print(f"ë‹µë³€: {result['response'][:200]}...")
        print(f"ì‹ ë¢°ë„: {result['confidence']:.2f}")
        print(f"ì‘ë‹µ ì‹œê°„: {result['response_time']:.2f}ì´ˆ")
        print(f"ì†ŒìŠ¤ ìˆ˜: {len(result['sources'])}")
        print("-" * 50)
=======
            print(f"ðŸ›ï¸ ìƒí’ˆ ë¬¸ì„œ {len(documents)}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
        
        return documents
    
    def search_documents(self, query: str, k: int = 3) -> List[Document]:
        """ë¬¸ì„œ ê²€ìƒ‰"""
        if not self.retriever:
            self.initialize_vector_store()
        
        try:
            results = self.retriever.invoke(query)
            return results[:k]
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def generate_response(self, query: str, context_docs: List[Document]) -> str:
        """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n\n".join([doc.page_content for doc in context_docs])

        # ì²´ì¸ ì‹¤í–‰ (LangSmith ìžë™ íŠ¸ë ˆì´ì‹±)
        chain = self.rag_prompt | self.llm

        try:
            # LangSmithì—ì„œ ì´ í˜¸ì¶œì„ ì¶”ì í•  ìˆ˜ ìžˆë„ë¡ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            response = chain.invoke(
                {
                    "context": context,
                    "question": query
                },
                config={
                    "metadata": {
                        "component": "RAGProcessor",
                        "operation": "generate_response",
                        "context_docs_count": len(context_docs),
                        "query_length": len(query)
                    }
                }
            )
            return response.content
        except Exception as e:
            print(f"âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ìž¬ ì‹œìŠ¤í…œì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ìž ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    def process_query(self, query: str) -> Dict[str, Any]:
        """ì¿¼ë¦¬ ì²˜ë¦¬ (ê²€ìƒ‰ + ì‘ë‹µ ìƒì„±)"""
        # ë¬¸ì„œ ê²€ìƒ‰
        relevant_docs = self.search_documents(query)
        
        if not relevant_docs:
            return {
                "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê³ ê°ì„¼í„°(1588-1234)ë¡œ ë¬¸ì˜í•´ì£¼ì‹œë©´ ë” ìžì„¸í•œ ë„ì›€ì„ ë°›ìœ¼ì‹¤ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.",
                "sources": [],
                "confidence": 0.0
            }
        
        # ì‘ë‹µ ìƒì„±
        response = self.generate_response(query, relevant_docs)
        
        # ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ
        sources = []
        for doc in relevant_docs:
            source_info = {
                "source": doc.metadata.get("source", "unknown"),
                "content_preview": doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content
            }
            if doc.metadata.get("source") == "product":
                source_info["product_id"] = doc.metadata.get("product_id")
            elif doc.metadata.get("source") == "faq":
                source_info["faq_id"] = doc.metadata.get("faq_id")
                source_info["category"] = doc.metadata.get("category")
            
            sources.append(source_info)
        
        return {
            "response": response,
            "sources": sources,
            "confidence": len(relevant_docs) / 3.0  # ê°„ë‹¨í•œ ì‹ ë¢°ë„ ê³„ì‚°
        }

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    rag = RAGProcessor()
    rag.initialize_vector_store()
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_queries = [
        "ë°°ì†¡ë¹„ëŠ” ì–¼ë§ˆì¸ê°€ìš”?",
        "ë¬´ì„  ì´ì–´í° ì‚¬ì–‘ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "ë°˜í’ˆì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?"
    ]
    
    for query in test_queries:
        print(f"\nðŸ” ì§ˆë¬¸: {query}")
        result = rag.process_query(query)
        print(f"ðŸ’¬ ë‹µë³€: {result['response']}")
        print(f"ðŸ“Š ì‹ ë¢°ë„: {result['confidence']:.2f}")
        print(f"ðŸ“š ì†ŒìŠ¤ ìˆ˜: {len(result['sources'])}")
>>>>>>> 98f88f8369a00fea011ba0112cbc9097e2eb5e55
